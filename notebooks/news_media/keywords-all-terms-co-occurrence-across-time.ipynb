{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we exlore the co-occurrence of keywords and all other terms that appear in the keywords' opinion context across the three lockdown-defined time windows to detect patterns of change in co-occurrence.\n",
    "\n",
    "Temporal windows are defined according to key events in the timeline of covid19 pandemic in the UK:\n",
    "\n",
    "- up to 23 March 2020 (excluded): pre-lockdown\n",
    "- 23 March to 10 May 2020: strict lockdown\n",
    "- 11 May 2020 onwards: post- strict lockdown (lockdown eases)\n",
    "\n",
    "\n",
    "\n",
    "We will:\n",
    "\n",
    "- [co-occurrence] For each temporal window, calculate the co-occurrence of keyword and word pairs as Dice coefficient \n",
    "- Identify changes in keyword and emerging words co-occurrence across the temporal windows.\n",
    "- Create networks of keyword co-occurrences for each of the three temporal windows and compare network and nodes characteristics across the three.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcol\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from src.utils import chain_functions\n",
    "from src.preproc_text import tokenise_sent, tokenise_word, remove_punctuation, remove_stopwords, flatten_irregular_listoflists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.news_media.get_keywords_trend import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = os.environ.get(\"DIR_DATA_INTERIM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA_EXTRA = os.environ.get(\"DIR_DATA_EXTRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prominence\n",
    "term_freqs_nm = \"kword_rawfreq.csv\"\n",
    "doc_freqs_nm = \"kword_yn_occurrence.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords to be excluded because of low frequency in the corpus\n",
    "EXCLUDE_KWORDS = ['behav_insight', 'behavioural_economist', 'behav_analysis', \n",
    "                  'chater', 'american_behav_scientists', 'irrational_econ', 'nudge_choice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we kept in some important words that though are not keywords\n",
    "KEY_WORDS = ['herd_immunity', 'behavioural_fatigue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_news = NewsArticles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(uk_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data = uk_news.data_raw.drop('full_text', axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data.opinion_context_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exclude opinion contexts that only contain non-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data = opinions_data[~opinions_data['kword'].isin(KEY_WORDS)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data.opinion_context_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN opinions_data = opinions_data[~opinions_data['kword'].isin(EXCLUDE_KWORDS)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only keep each unique opinion context once (now there are as many instances as how many subkeywords they contain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq = opinions_data[['article_id', 'title', 'opinion_context',\n",
    "                                    'pub_date_dt']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opinions_data_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opinions_data_uniq.sort_values(\"opinion_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get nouns and adjectives only from opinion context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_symbols(text: str) -> str:\n",
    "    symbols = [\n",
    "        \"©\", \"\\xad\", \"•\", \"…\", \"●\", \"“\", \"”\", \"•'\", \"\\u200b\", \"£\", \"'\", \"'s\",\n",
    "        \"·\", \"»\", \"com/\"\n",
    "    ]\n",
    "    for symb in symbols:\n",
    "        if symb not in text:\n",
    "            continue\n",
    "        text = text.replace(symb, \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KWORDS = CONFIG['Actors'] + CONFIG['BehavSci'] + CONFIG['Behav_ins'] + CONFIG[\n",
    "    'Behav_chan'] + CONFIG['Behav_pol'] + CONFIG['Behav_anal'] + CONFIG[\n",
    "        'Psych'] + CONFIG['Econ_behav'] + CONFIG['Econ_irrational'] + CONFIG[\n",
    "            'Nudge'] + CONFIG['Nudge_choice'] + CONFIG['Nudge_pater']\n",
    "OTHER_IMPORTANT_WORDS = CONFIG['Covid'] + CONFIG['Fatigue'] + CONFIG['Immunity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be removed as we know they will co-occur with the related surnames without adding anything to the analysis\n",
    "RELEVANT_FIRST_NAMES = ['susan', 'david', 'stephen', 'boris', \n",
    "                        'nick', 'daniel', 'robert', 'richard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only keep NOUNS, ADJECTIVES and keywords\n",
    "# Assumptions: anything else is not adding to the content of the articles\n",
    "TEXT_PREPROC_PIPE = chain_functions(\n",
    "    lambda x: x.lower()\n",
    "    ,remove_special_symbols\n",
    "    ,lambda x: re.sub(r'[.]+(?![0-9])', r' ', x)\n",
    "    ,tokenise_sent\n",
    "    ,tokenise_word\n",
    "    ,lambda x: [[\n",
    "        word for (word, pos) in pos_tag(sent) if \n",
    "        (pos.startswith(\"N\")) \n",
    "        or (pos.startswith(\"J\")) \n",
    "        or (word in KWORDS) or (word in OTHER_IMPORTANT_WORDS)\n",
    "    ] for sent in x]\n",
    "    ,lambda x: [[word for word in sent if word not in RELEVANT_FIRST_NAMES] for sent in x]\n",
    "    ,remove_punctuation\n",
    "    ,remove_stopwords\n",
    "    ,flatten_irregular_listoflists\n",
    "    ,list\n",
    "    ,lambda x: ' '.join(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert opinion_context variable from str to tuple, only extract text\n",
    "opinions_data_uniq[\"opinion_context_text\"] = [literal_eval(opinion)[1] for opinion in \n",
    "                                              opinions_data_uniq.opinion_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep nouns and adjectives, plus additional data cleaning\n",
    "opinions_data_uniq[\"opinion_context_nj\"] = [TEXT_PREPROC_PIPE(opinion) for opinion in \n",
    "                                            opinions_data_uniq.opinion_context_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace common phrases made of separate words with their combination\n",
    "e.g., \"university college london\" > \"university_college_london\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the bigram and trigram models based on co-collocation statistical patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect phrases, based on collected collocation counts. Adjacent words that appear together more frequently than expected are joined together with the `_` character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the articles' whole texts to learn collocation as the model requires a big enough sample to infer statistically reliable collocation patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the articles texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the keywords from the collocation as otherwise, they may be replaced with a combination of \n",
    "# keyword + other common word the keyword tends to appear with (e.g., \"prof_michie\")\n",
    "\n",
    "ARTICLES_PREPROC_PIPE = chain_functions(\n",
    "    lambda x: x.lower()\n",
    "    ,tokenise_sent\n",
    "    ,tokenise_word\n",
    "    ,lambda x: [[\n",
    "        word for word in sent if ((word not in KWORDS) and (word not in OTHER_IMPORTANT_WORDS)\n",
    "                                 and (word not in RELEVANT_FIRST_NAMES) )\n",
    "    ] for sent in x]\n",
    "    ,flatten_irregular_listoflists\n",
    "    ,list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As corpus to learn the common phrases, we use the text from all the articles\n",
    "corpus = [ARTICLES_PREPROC_PIPE(article) for article in uk_news.data_raw['full_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq[\"opinion_context_nj\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Phrases)\n",
    "#dir(Phraser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gensim model: collocation-statistics-based Phrases process.\n",
    "\n",
    "Automatically detect common phrases (multiword expressions) from a stream of texts as token strings.\n",
    "\n",
    "The phrases are collocations (frequently co-occurring tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "phrases = Phrases(corpus,\n",
    "                min_count=10, #ignore all words and bigrams with total collected count lower than this\n",
    "                threshold=10.0 #represents a threshold for forming the phrases (higher means fewer phrases). \n",
    "                  #A phrase of words a and b is accepted if \n",
    "                  #(cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold, \n",
    "                  #where N is the total vocabulary size. Bydefault it value is 10.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Phrases(bigram[corpus], \n",
    "                  min_count=10,\n",
    "                 threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigram[word_tokenize(opinions_data_uniq[\"opinion_context_nj\"][10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram[word_tokenize(opinions_data_uniq[\"opinion_context_nj\"][10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq[\"opinion_context_nj\"][39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram[word_tokenize(opinions_data_uniq[\"opinion_context_nj\"][39])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram[word_tokenize(opinions_data_uniq[\"opinion_context_nj\"][39])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram[word_tokenize(\"she teaches at university college london\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram[word_tokenize(\"prof michie teaches at university college london\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram[word_tokenize(\"prof michie teaches at university college london and public health england\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram[word_tokenize(\"prof susan michie teaches at university college london and public health england\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign a new unique id for each opinion context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq[\"context_id\"] = range(1, opinions_data_uniq.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace sub-keywords with higher level keyword in opinion context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq[\"opinion_context_nj_tok\"] = [word_tokenize(opinion) for opinion in \n",
    "                                                opinions_data_uniq.opinion_context_nj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function and sub-keyword to keyword dictionary (loaded as part of the src.get_keyword_trend)\n",
    "SUBKEYWORDS_TO_KEYWORDS_DICT = expand_dict(SUBKEYWORDS_TO_KEYWORDS_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opinions_data_uniq[\"opinion_context_nj_tok_kw\"] = [[SUBKEYWORDS_TO_KEYWORDS_DICT.get(word, word) for word in \n",
    "                                                    opinion] for opinion in opinions_data_uniq.opinion_context_nj_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace common two-word and three-word phrases with correspoding bi- and tri-gram \n",
    "\n",
    "Based on collocation-statistics learnt for the whole corpus of articles above,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq[\"opinion_context_nj_tok_kw_tri\"] = [trigram[opinion_tok] for opinion_tok in \n",
    "                                                       opinions_data_uniq.opinion_context_nj_tok_kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_data_uniq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute word document (opinion context) frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: from list of token words, to token string\n",
    "opinions_data_uniq['opinion_context_nj_tok_kw_tri_str'] = opinions_data_uniq[\"opinion_context_nj_tok_kw_tri\"].apply(\n",
    "    lambda text: ' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "#opinions_data_uniq[[\"opinion_context_nj_tok_kw_tri\", \"opinion_nj_kw_str\"]][:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in opinions_data_uniq['opinion_context_nj_tok_kw_tri_str'] if \"susan\" in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_words_raw_tf(df: pd.DataFrame,\n",
    "                            text_col='') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the document-term frequency matrix for all the unigram nouns in the preprocessing texts.\n",
    "\n",
    "        Args:\n",
    "            df: pandas.Dataframe. Must contain columns \"pub_date_dt\", \"article_id\", \"context_id\"\n",
    "            text_col: name of column containing the text for which to compute TD matrix. The dtype must be string.\n",
    "\n",
    "        Returns:\n",
    "            The document-term frequency matrix for all the unigrams in the preprocessed corpus of articles.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not all([item in df.columns for item in [\"pub_date_dt\", \"article_id\", \"context_id\"]]):\n",
    "            raise KeyError\n",
    "            \n",
    "        vec = CountVectorizer(stop_words=None,\n",
    "                              tokenizer=word_tokenize,\n",
    "                              ngram_range=(1, 1),\n",
    "                              token_pattern=r\"(?u)\\b\\w\\w+\\b\")\n",
    "        \n",
    "        results_mat = vec.fit_transform(df[text_col])\n",
    "        \n",
    "        # sparse to dense matrix\n",
    "        results_mat = results_mat.toarray()\n",
    "\n",
    "        # get the feature names from the already-fitted vectorizer\n",
    "        vec_feature_names = vec.get_feature_names()\n",
    "\n",
    "        # make a table with word frequencies as values and vocab as columns\n",
    "        out_df = pd.DataFrame(data=results_mat, columns=vec_feature_names)\n",
    "        \n",
    "        # add opinion context id and pub date as indexes\n",
    "        # we use the property of CountVectorizer to keep the order of the original texts\n",
    "        out_df[\"pub_date_dt\"] = df[\"pub_date_dt\"].tolist()\n",
    "        out_df[\"article_id\"] = df[\"article_id\"].tolist()\n",
    "        out_df[\"context_id\"] = df[\"context_id\"].tolist()\n",
    "        out_df[text_col] = df[text_col].tolist()\n",
    "\n",
    "        out_df.set_index(['pub_date_dt', 'article_id', 'context_id', text_col ],\n",
    "                         append=True,\n",
    "                         inplace=True)\n",
    "\n",
    "\n",
    "        \n",
    "        return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opinions_data_uniq[['article_id', 'context_id','pub_date_dt','opinion_context_nj_tok_kw']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opinions_termfreqs = compute_words_raw_tf(df=opinions_data_uniq, text_col=\"opinion_context_nj_tok_kw_tri_str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check keywords frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks\n",
    "\"susan\" in opinions_termfreqs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_termfreqs[['american_behav_scientists', 'halpern', 'michie', 'chater', 'spi-b', \n",
    "                    'behav_insights_team', 'behav_science', 'behav_insight', 'behav_change', \n",
    "                    'behav_scientist', 'behav_analysis', 'psychology', 'psychologist', \n",
    "                    'behav_econ', 'behavioural_economist','nudge', 'herd_immunity', 'behavioural_fatigue']].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(os.path.join(DIR_DATA, term_freqs_nm)).iloc[:, 4:].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate document frequency for keywords and words\n",
    "\n",
    "This is a value of 1 if the (key)word appears in the opinion context, regardless of how many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_termfreqs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence = opinions_termfreqs.applymap(lambda cell: 1 if cell > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence[['american_behav_scientists', 'halpern', 'michie', 'chater', 'spi-b', \n",
    "                    'behav_insights_team', 'behav_science', 'behav_insight', 'behav_change', \n",
    "                    'behav_scientist', 'behav_analysis', 'psychology', 'psychologist', \n",
    "                    'behav_econ', 'behavioural_economist','nudge']].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group data into time windows\n",
    "\n",
    "According to dates: before 23 March, from 23 March to 10 May, from 11 May onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_weeks(date):\n",
    "    \"\"\"Assigns and labels weeks to a time window.\"\"\"\n",
    "    if date <= datetime.strptime(\"2020-03-22\", '%Y-%m-%d'):\n",
    "        return \"before-lockdown\"\n",
    "    if (date > datetime.strptime(\"2020-03-22\", '%Y-%m-%d')) and (date <= datetime.strptime(\"2020-05-10\", '%Y-%m-%d')):\n",
    "        return \"lockdown\"\n",
    "    if date > datetime.strptime(\"2020-05-10\", '%Y-%m-%d'):\n",
    "        return \"post-lockdown\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence[\"time_window\"] = opinions_term_yn_occurrence.index.get_level_values('pub_date_dt').map(label_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence.set_index([opinions_term_yn_occurrence.index, \"time_window\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words (other than keywords and term of conceptual interest, i.e., \"behavioural fatigue\" and  \"herd immunity\") with low document frequency\n",
    "\n",
    "Words appeasring in less than 20 (TBC) opinion contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(SUBKEYWORDS_TO_KEYWORDS_DICT.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KEEP_KEY_WORDS = list(set(SUBKEYWORDS_TO_KEYWORDS_DICT.values())) + KEY_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_KWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_KEY_WORDS = [word for word in KEEP_KEY_WORDS if word not in EXCLUDE_KWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of words (other than selected keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence.drop(KEEP_KEY_WORDS, axis=1).sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many non-keywords have a document frequency below vs. above 20? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docfreqs_nonkeywords = opinions_term_yn_occurrence.drop(KEEP_KEY_WORDS, axis=1).sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docfreqs_nonkeywords[docfreqs_nonkeywords > 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docfreqs_nonkeywords[docfreqs_nonkeywords <= 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(docfreqs_nonkeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(docfreqs_nonkeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"median: {np.percentile(docfreqs_nonkeywords, [50])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q75, q25 = np.percentile(docfreqs_nonkeywords, [75 ,25])\n",
    "iqr = q75 - q25\n",
    "print(f\"interquartile range: {iqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above20_terms = docfreqs_nonkeywords[docfreqs_nonkeywords > 20].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence[above20_terms].groupby('time_window').sum().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only keep terms that are either our selected keywords or words with a document frequency above 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_toolow_docfreq = docfreqs_nonkeywords[docfreqs_nonkeywords <= 20].reset_index()[\"index\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_toolow_docfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence.drop(words_toolow_docfreq, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate before-lockdown vs lockdown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_freqs_before = opinions_term_yn_occurrence[\n",
    "    opinions_term_yn_occurrence.index.get_level_values('time_window').isin(['before-lockdown'])]\n",
    "doc_freqs_lock = opinions_term_yn_occurrence[\n",
    "    opinions_term_yn_occurrence.index.get_level_values('time_window').isin(['lockdown'])]\n",
    "doc_freqs_post = opinions_term_yn_occurrence[\n",
    "    opinions_term_yn_occurrence.index.get_level_values('time_window').isin(['post-lockdown'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many opinion contexts per time window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_term_yn_occurrence.groupby(\"time_window\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dice coefficient\n",
    "\n",
    "We define co-occurrence as two words appearing in the same opinion context, regardless of how many times each appears in the articles. So our measure of co-occurrence is based on document co-occurrence where document is the opinion context.\n",
    "\n",
    "We normalise co-occurrence by using the Dice coefficient, which is a used in corpus linguistics and should not inflate the importance of co-occurrence for keywords with a very low appearence count in the corpus. That is, words that are frequent by themselves tend to have frequent relations to other words. Other metrics like Dice coefficiet, mutual information orlog-likelihood ratio, will tell if a relation is more frequent than one would expect given the frequency of the individual words.\n",
    "\n",
    "- The co-occurrence relationship between any two keywords was expressed by the Dice coefficient in information theory (pseudocode shown below), describing the strength of association between these two keywords. \n",
    "- The main reason for choosing mutual information instead of selecting the number of frequent words could be analysed by the following process, ADD\n",
    "- Note that the co-occurrence of two keywords can be relatively small, but if they almost always appeared at the same time, their Dice coefficient will be higher, as they were considered to be in a co-occurrence relationship.\n",
    "\n",
    "Ref: \n",
    "https://onlinelibrary.wiley.com/doi/pdf/10.1002/ecj.10347\n",
    "\n",
    "https://www.aclweb.org/anthology/C12-2049.pdf\n",
    "\n",
    "https://www.aclweb.org/anthology/J05-4002.pdf\n",
    "\n",
    "`Dice coefficient = (2 * count(w1, w2)) / (count(w1) + count(w2))`\n",
    "\n",
    "The number of co-occurrences multiplied by two divided by the sum of the two keywords' individual document occurrences.\n",
    "\n",
    "E.g., Given the following document occurrences and co-occurrence: keyword A : 10, keyword B : 6, co-occurrence :4,\n",
    "the Dice coefficient representing the co-occurrence of A and B is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*4/(10+6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared this to a case where keyword A : 10, keyword B : 6, co-occurrence : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*6/(10+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def calc_dice(yn_occurence_data: pd.DataFrame, cooccurence_threshold: int=None, prefix=\"\"):\n",
    "    \n",
    "    # keyword document occurrence\n",
    "    kword_docfreqs = yn_occurence_data.sum(axis=0)\n",
    "    \n",
    "    # keywords co-occurrence matrix\n",
    "    kword_cooccurences = yn_occurence_data.values.T.dot(yn_occurence_data.values)\n",
    "    np.fill_diagonal(kword_cooccurences, 0)\n",
    "    kwords = yn_occurence_data.columns\n",
    "    kword_cooccurences = pd.DataFrame(kword_cooccurences, index=kwords, columns=kwords)\n",
    "    kword_cooccurences = kword_cooccurences.stack()\n",
    "    \n",
    "    # if specified, only keep word pairs whose co-occurrence is about specified value\n",
    "    if cooccurence_threshold:\n",
    "        kword_cooccurences = kword_cooccurences[kword_cooccurences >= cooccurence_threshold].copy()\n",
    "        \n",
    "    # extract list of words\n",
    "    words_list = list(set([tup[0] for tup in kword_cooccurences.index.values] + \n",
    "                           [tup[1] for tup in kword_cooccurences.index.values]))\n",
    "    \n",
    "    # filter word doc frequency accordigly\n",
    "    kword_docfreqs = kword_docfreqs[words_list].copy()\n",
    "    \n",
    "    \n",
    "    def _dice(w1, w2):\n",
    "        # print(f\"{w1}: {kword_docfreqs[w1]}\")\n",
    "        # print(f\"{w2}: {kword_docfreqs[w2]}\")\n",
    "        # print(f\"coocc: {kword_cooccurences[w1][w2]}\")\n",
    "        try:\n",
    "            return (2 * kword_cooccurences[w1][w2]) / (kword_docfreqs[w1] + kword_docfreqs[w2])\n",
    "        except (ValueError, ZeroDivisionError) as err: # one of the two individual counts are 0\n",
    "            return np.nan\n",
    "        except KeyError as err:\n",
    "            print(f\"{w1} and {w2} do not co-occur. Computation of Dice coefficient is skipped.\")\n",
    "            pass\n",
    "        \n",
    "    def dice(kwords_list: list) -> list:\n",
    "        coefs = []\n",
    "        for pair in combinations(kwords_list, r=2):\n",
    "            try:\n",
    "                coefs.append((*pair, _dice(*pair), kword_cooccurences[pair[0]][pair[1]], \n",
    "                              kword_docfreqs[pair[0]], kword_docfreqs[pair[1]] ))\n",
    "            except KeyError as err:\n",
    "                print(f\"{pair} do not co-occur. Computation of Dice coefficient is skipped.\")\n",
    "                pass  \n",
    "        return coefs\n",
    "    \n",
    "    dices = dice(kwords_list=words_list)\n",
    "    dices_df = pd.DataFrame(dices, columns=['source', 'target', f'{prefix}_weight', f'{prefix}_co-occ', f'{prefix}_source_docfreq', f'{prefix}_target_docfreq'])\n",
    "    \n",
    "    return dices_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "before_dice_coefs = calc_dice(yn_occurence_data=doc_freqs_before, \n",
    "                              cooccurence_threshold=10, \n",
    "                              prefix=\"bef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_dice_coefs = calc_dice(yn_occurence_data=doc_freqs_lock, \n",
    "                            cooccurence_threshold=10, \n",
    "                            prefix=\"lock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dice_coefs = calc_dice(yn_occurence_data=doc_freqs_post, \n",
    "                            cooccurence_threshold=10, \n",
    "                            prefix=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter results so that at least one of the word in the pair is a selected keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_KEYWORDS = [kword for kword in KEEP_KEY_WORDS if kword not in ['herd_immunity', 'behavioural_fatigue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dice_coefs = before_dice_coefs[(before_dice_coefs['source'].isin(SELECTED_KEYWORDS)) |  \n",
    "                  (before_dice_coefs['target'].isin(SELECTED_KEYWORDS))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_dice_coefs = lock_dice_coefs[(lock_dice_coefs['source'].isin(SELECTED_KEYWORDS)) |  \n",
    "                  (lock_dice_coefs['target'].isin(SELECTED_KEYWORDS))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dice_coefs = post_dice_coefs[(post_dice_coefs['source'].isin(SELECTED_KEYWORDS)) |  \n",
    "                  (post_dice_coefs['target'].isin(SELECTED_KEYWORDS))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How has co-occurrence (as Dice coefficient) evolved before vs. during vs. post lockdown\n",
    "\n",
    "CHECK : is Dice coefficient a fair representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dice_coefs.sort_values('bef_weight', ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_dice_coefs.sort_values('lock_weight', ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dice_coefs.sort_values('post_weight', ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the coefficients from the three time blocks to compare them more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coefs = before_dice_coefs.merge(lock_dice_coefs, \n",
    "                                     how='outer', \n",
    "                                     on = ['source', 'target']).merge(post_dice_coefs, \n",
    "                                                                      how='outer', \n",
    "                                                                      on = ['source', 'target']) \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dice_coefs[['source', 'target', 'bef_weight', 'lock_weight', 'post_weight']][40:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 co-occurrent keyword pairs in each time window and how their co-occurrence has changed across the three time blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the 10 pairs of keyword with the highest Dice coefficients in each of the three time windows, and see how their co-occurrence (as Dice coefficient) has evolved across the three periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_top10_coocurring_pairs = before_dice_coefs.sort_values('bef_weight', ascending=False)[:10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_top10_coocurring_pairs = lock_dice_coefs.sort_values('lock_weight', ascending=False)[:10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_top10_coocurring_pairs = post_dice_coefs.sort_values('post_weight', ascending=False)[:10].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look (some pairs will be present in more than one time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_top10_coocurring_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_top10_coocurring_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_top10_coocurring_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coocurring_pairs = before_top10_coocurring_pairs.merge(lock_top10_coocurring_pairs, \n",
    "                                                           how='outer', \n",
    "                                                           on = ['source', 'target']).merge(post_top10_coocurring_pairs, \n",
    "                                                                                            how='outer', \n",
    "                                                                                            on = ['source', 'target']) \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coocurring_pairs[['source', 'target', 'bef_weight', 'lock_weight', 'post_weight']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualise how their pattern and strength of co-occurrence has changed across the three time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coefs_matrix = np.array(top_coocurring_pairs[['bef_weight', 'lock_weight', 'post_weight']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coocurring_pairs['keywords_pair'] = [s + \"  |  \" + t for s, t in \n",
    "                                         zip(top_coocurring_pairs['source'], top_coocurring_pairs['target'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coocurring_pairs[['keywords_pair', 'bef_weight', 'lock_weight', 'post_weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Annotated Heatmap\n",
    "z_text = np.array(pd.DataFrame(np.around(dice_coefs_matrix, decimals=2)).fillna(''), dtype=str)\n",
    "\n",
    "# Set Colorscale\n",
    "colorscale=[[0.0, 'rgb(247, 232, 246)'], [1.0, 'rgb(255, 77, 148)']]\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=dice_coefs_matrix,\n",
    "    x=['before lockdown', 'during lockdown', 'post lockdown'],\n",
    "    y=top_coocurring_pairs['keywords_pair'].tolist(), \n",
    "    annotation_text=z_text, \n",
    "    text=z_text,\n",
    "    colorscale=colorscale, \n",
    "    hoverinfo='z',\n",
    "    font_colors=['black'])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='',\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    xaxis=dict(showline=True, linewidth=1, linecolor='black', mirror=True),\n",
    "    yaxis=dict(visible=True,autorange='reversed', showline=True, linewidth=1, linecolor='black', mirror=True,\n",
    "             ),\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that started, ended or continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we summarise the trend in co-occurrence to identify whether the co-occurrence between two keywords:\n",
    "- started / ended / stayed during lockdown (compared to before lockdown)\n",
    "- started / ended / stayed post lockdown (compared to during lockdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_in_cooccurrence(score1, score2):\n",
    "    if ((score1 == 0.0) or (np.isnan(score1))) and ((score2 == 0.0) or (np.isnan(score2))):\n",
    "        return \"none\"\n",
    "    if ((score1 != 0.0) and (~np.isnan(score1))) and ((score2 != 0.0) and (~np.isnan(score2))):\n",
    "        return \"stayed\"\n",
    "    if ((score1 != 0.0) and (~np.isnan(score1))) and ((score2 == 0.0) or (np.isnan(score2))):\n",
    "        return \"ended\"\n",
    "    if ((score1 == 0.0) or (np.isnan(score1))) and ((score2 != 0.0) and (~np.isnan(score2))):\n",
    "        return \"started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coefs['coocc_trend_lock_vs_before'] = dice_coefs.apply(lambda row: \n",
    "                                                trend_in_cooccurrence(row['bef_weight'], row['lock_weight']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coefs['coocc_trend_post_vs_lock'] = dice_coefs.apply(lambda row: \n",
    "                                                trend_in_cooccurrence(row['lock_weight'], row['post_weight']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coefs[['source', 'target', 'bef_weight', 'lock_weight', \n",
    "            'post_weight', 'coocc_trend_lock_vs_before', 'coocc_trend_post_vs_lock']].sort_values(\"bef_weight\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that started during lock-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(dice_coefs[dice_coefs.coocc_trend_lock_vs_before == \"started\"][['source', 'target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that ended during lock-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dice_coefs[dice_coefs.coocc_trend_lock_vs_before == \"ended\"][['source', 'target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that remained during lock-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dice_coefs[dice_coefs.coocc_trend_lock_vs_before == \"stayed\"][['source', 'target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that started post lock-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dice_coefs[dice_coefs.coocc_trend_post_vs_lock == \"started\"][['source', 'target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that ended post lock-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dice_coefs[dice_coefs.coocc_trend_post_vs_lock == \"ended\"][['source', 'target']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrences that remained post lock-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dice_coefs[dice_coefs.coocc_trend_post_vs_lock == \"stayed\"][['source', 'target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network based on Dice coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the co-occurrence network of keywords in the three time windows.\n",
    "\n",
    "- **Nodes** represent keywords.\n",
    "- **Edges** represent their co-occurrence strength (Dice coefficient)\n",
    "- **The node size** represent the value of the node degree, which means the number of neighbor nodes connected to the node directly. The degree captures the importance of of the keyword in the network, a higher degree indictes a highly connected keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csfont = {'fontname':'Helvetica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Louvain modularity algorithm to detect communities of co-occurring words in our networks as implemented by the `python-louvain` Python package (https://python-louvain.readthedocs.io/en/latest/api.html). The alogirthm returns the partition of highest modularity, i.e. the highest partition of the dendrogram generated by the Louvain algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Louvain algorithm works by maximising modularity (Blondel et al., 2008). Modularity measures the density of connections within communties compared to the density of connections between communities, it takes on values between -1 and 1, and a higher value represents better community definition (Newman & Girvan, 2004). See supplementary material for additional information.\n",
    "\n",
    "The Louivain algorithm γ > 0 is a resolution parameter. Higher resolutions lead to more communities, while lower resolutions lead to fewer communities. We iterated over possible values of the resolution parameters r (start 0.1, end 5, step=0.2) and opted for the lowest value of r that led to the maximum modularity value.\n",
    "\n",
    "Blondel, V. D., Guillaume, J. L., Lambiotte, R., Lefebvre, E. (2008), Fast unfolding of communities in large networks, Journal of Statistical Mechanics: Theory and Experiment, Nr.10, P10008\n",
    "\n",
    "Found to be one of the fastest and best performing algorithms in comparative analyses: Lancichinetti, A. & Fortunato, S. Community detection algorithms: A comparative analysis. Phys. Rev. E 80, 056117, https://doi.org/10.1103/PhysRevE.80.056117 (2009).\n",
    "Yang, Z., Algesheimer, R. & Tessone, C. J. A Comparative Analysis of Community Detection Algorithms on Artificial Networks. Sci. Rep. 6, 30750, https://doi.org/10.1038/srep30750 (2016).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN cases and 0.0 values\n",
    "before_dice_coefs.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dice_coefs = before_dice_coefs[before_dice_coefs.bef_weight > 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dice coefficient above a certain threshols?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dice_graph = nx.from_pandas_edgelist(before_dice_coefs[['source', 'target', 'bef_weight']], edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at one\n",
    "print(nx.to_dict_of_dicts(before_dice_graph).get('michie'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_louvain.generate_dendrogram(before_dice_graph, weight='bef_weight', resolution=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over a range of values for gamma, the resolution parameter, and detect the value leading to the higher modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_resolution_iter = {}\n",
    "for r in [x / 10.0 for x in range(1, 50, 1)]:\n",
    "    comp = community_louvain.best_partition(before_dice_graph, weight='bef_weight',  resolution=r)\n",
    "    Q = community_louvain.modularity(comp, before_dice_graph, weight='bef_weight')\n",
    "    bef_resolution_iter[r] = Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_resolution_iter    #1.0: 0.505713233567339,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before\n",
    "\n",
    "bef_partition = community_louvain.best_partition(before_dice_graph, weight='bef_weight',  resolution=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Modularity (before network): {round(community_louvain.modularity(bef_partition, before_dice_graph, weight='bef_weight'), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nodes' betweeness\n",
    "\n",
    "to use as node size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_betweenness_dict = nx.betweenness_centrality(\n",
    "    before_dice_graph, \n",
    "    weight='bef_weight'\n",
    "    ) \n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(before_dice_graph, before_betweenness_dict, 'betweenness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights, we'll use them for plotting\n",
    "before_dice_graph_weights = list(nx.get_edge_attributes(before_dice_graph,'bef_weight').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nodes' degree (alternative to use as node size)\n",
    "before_dice_graph_degrees = dict(nx.degree(before_dice_graph, weight='bef_weight'))\n",
    "#before_dice_graph_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color the edges, starting from a darker tone of gray\n",
    "#https://stackoverflow.com/questions/26102515/select-starting-color-in-matplotlib-colormap\n",
    "\n",
    "lvTmp = np.linspace(0.1,1.0,len(before_dice_graph_weights)-1)\n",
    "lvTmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmTmp = plt.cm.Greys(lvTmp)\n",
    "newCmap = mcol.ListedColormap(cmTmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap(cm.colors.ListedColormap([\"lightblue\", \"peru\",  \"lightgreen\", \"fuchsia\", \"orange\"]), \n",
    "                   max(bef_partition.values()) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (80,80))\n",
    "\n",
    "nx.draw_spring(before_dice_graph, \n",
    "               k=10,\n",
    "               with_labels=True, \n",
    "               edge_color=before_dice_graph_weights,\n",
    "               width=[v*30 for v in before_dice_graph_weights],\n",
    "               #node_color='lightgray',\n",
    "               node_color=list(bef_partition.values()),\n",
    "               cmap=cmap,\n",
    "               linewidths=2,\n",
    "               #node_size=[v * 10000 for v in before_dice_graph_degrees.values()],\n",
    "               node_size=[(v * 100000)+2000 for v in before_betweenness_dict.values()],\n",
    "               font_size=100,\n",
    "               font_type=\"Helvetica\",\n",
    "               font_color=\"black\",\n",
    "               font_weight=3,\n",
    "               edge_cmap=newCmap,\n",
    "               edge_vmin=0,\n",
    "               seed=20\n",
    "              )\n",
    "\n",
    "plt.title(\"Before strict lockdown\".upper(), fontsize=150, **csfont)\n",
    "plt.axis('off')\n",
    " \n",
    "plt.savefig('before_network.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN cases and 0.0 values\n",
    "lock_dice_coefs.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_dice_coefs = lock_dice_coefs[lock_dice_coefs.lock_weight > 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_dice_graph = nx.from_pandas_edgelist(lock_dice_coefs[['source', 'target', 'lock_weight']], edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nodes\n",
    "#sorted(lock_dice_graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at one\n",
    "print(nx.to_dict_of_dicts(lock_dice_graph).get('michie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights, we'll use them for plotting\n",
    "lock_dice_weights = list(nx.get_edge_attributes(lock_dice_graph,'lock_weight').values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over value of the resolution parameter to find the one leading to highest modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_resolution_iter = {}\n",
    "for r in [x / 10.0 for x in range(1, 50, 1)]:\n",
    "    comp = community_louvain.best_partition(lock_dice_graph, weight='lock_weight',  resolution=r)\n",
    "    Q = community_louvain.modularity(comp, lock_dice_graph, weight='lock_weight')\n",
    "    lock_resolution_iter[r] = Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_resolution_iter    #1.0: 0.48165320554268937,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during\n",
    "lock_partition = community_louvain.best_partition(lock_dice_graph, weight='lock_weight',  resolution=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Modularity (before network): {round(community_louvain.modularity(lock_partition, lock_dice_graph, weight='lock_weight'), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nodes' betweeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nodes' degree (alternative to use as node size)\n",
    "lock_dice_graph_degrees = dict(nx.degree(lock_dice_graph, weight='lock_weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nodes' betweeness to use as node size\n",
    "lock_betweenness_dict = nx.betweenness_centrality(\n",
    "    lock_dice_graph, \n",
    "    weight='lock_weight'\n",
    "    ) \n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(lock_dice_graph, lock_betweenness_dict, 'betweenness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let color of edges start from a darker tone of gray\n",
    "#https://stackoverflow.com/questions/26102515/select-starting-color-in-matplotlib-colormap\n",
    "\n",
    "lvTmp_lock = np.linspace(0.1,1.0,len(lock_dice_weights)-1)\n",
    "lvTmp_lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmTmp_lock = plt.cm.Greys(lvTmp_lock)\n",
    "newCmap_lock = mcol.ListedColormap(cmTmp_lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap(cm.colors.ListedColormap([\"lightblue\", \"crimson\",  \"lightgreen\", \"pink\", \"orange\"]), \n",
    "                   max(lock_partition.values()) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (80,80))\n",
    "\n",
    "nx.draw_spring(lock_dice_graph, \n",
    "               k=10,\n",
    "               with_labels=True, \n",
    "               edge_color=lock_dice_weights,\n",
    "               width=[v*30 for v in lock_dice_weights],\n",
    "               #node_color='lightgray',\n",
    "               node_color=list(lock_partition.values()),\n",
    "               cmap=cmap,\n",
    "               linewidths=2,\n",
    "               #node_size=[v * 10000 for v in before_dice_graph_degrees.values()],\n",
    "               node_size=[(v * 100000)+2000 for v in lock_betweenness_dict.values()],\n",
    "               font_size=100,\n",
    "               font_type=\"Helvetica\",\n",
    "               font_color=\"black\",\n",
    "               font_weight=3,\n",
    "               edge_cmap=newCmap,\n",
    "               edge_vmin=0,\n",
    "               seed=28\n",
    "              )\n",
    "\n",
    "plt.title(\"During strict lockdown\".upper(), fontsize=150, **csfont)\n",
    "plt.axis('off')\n",
    " \n",
    "plt.savefig('lock_network.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JUST FOR PLOTTING PURPOSES: Very hard to visualise correctly, otherwise. So we will plot the two sub plots separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data1 = lock_dice_coefs[['source', 'target', 'lock_weight']][\n",
    "    (lock_dice_coefs['source'].isin(['halpern', 'behav_insights_team', 'dr'])) |  \n",
    "                  (lock_dice_coefs['target'].isin(['halpern', 'behav_insights_team', 'dr']))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_network1 = nx.from_pandas_edgelist(sub_data1, edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.to_dict_of_dicts(sub_network1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data2 = lock_dice_coefs[['source', 'target', 'lock_weight']][\n",
    "    ~(lock_dice_coefs['source'].isin(['halpern', 'behav_insights_team', 'dr'])) & \n",
    "                  ~(lock_dice_coefs['target'].isin(['halpern', 'behav_insights_team', 'dr']))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_network2 = nx.from_pandas_edgelist(sub_data2, edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights, we'll use them for plotting\n",
    "lock_dice_weights_2 = list(nx.get_edge_attributes(sub_network2,'lock_weight').values())\n",
    "lock_dice_weights_1 = list(nx.get_edge_attributes(sub_network1,'lock_weight').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_betweenness_dict_2 = dict((k, lock_betweenness_dict[k]) for k in lock_betweenness_dict.keys() \n",
    "     if k not in ['halpern', 'behav_insights_team', 'dr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_betweenness_dict_1 = dict((k, lock_betweenness_dict[k]) for k in ['halpern', 'behav_insights_team', 'dr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_partition_2 = dict((k, lock_partition[k]) for k in lock_partition.keys() \n",
    "     if k not in ['halpern', 'behav_insights_team', 'dr']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvTmp_lock_2 = np.linspace(0.1,1.0,len(lock_dice_weights_2)-1)\n",
    "\n",
    "cmTmp_lock_2 = plt.cm.Greys(lvTmp_lock_2)\n",
    "newCmap_lock_2 = mcol.ListedColormap(cmTmp_lock_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap2 = cm.get_cmap(cm.colors.ListedColormap([\"orange\", \"peru\",  \"lightgreen\", \"fuchsia\"]), \n",
    "                   max(bef_partition.values()) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (80,80))\n",
    "\n",
    "nx.draw_spring(sub_network2, \n",
    "               k=10,\n",
    "               with_labels=True, \n",
    "               edge_color=lock_dice_weights_2,\n",
    "               #width=8,\n",
    "               width=[v*30 for v in lock_dice_weights_2],\n",
    "               node_color=list(lock_partition_2.values()),\n",
    "               cmap=cmap2,\n",
    "               linewidths=1,\n",
    "               #node_size=[v * 10000 for v in lock_dice_graph_degrees.values()],\n",
    "               node_size=[(v * 100000)+2000 for v in lock_betweenness_dict_2.values()],\n",
    "               font_size=100,\n",
    "               font_type=\"Helvetica\",\n",
    "               font_color='black',\n",
    "               font_weight=3,\n",
    "               edge_cmap=newCmap_lock_2,\n",
    "               edge_vmin=0,\n",
    "               seed=37\n",
    "              )\n",
    "\n",
    "plt.title(\"During strict lockdown\".upper(), fontsize=150, **csfont)\n",
    "plt.axis('off')\n",
    " \n",
    "plt.savefig('lock_network_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After strict lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dice_coefs.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dice_coefs = post_dice_coefs[post_dice_coefs.post_weight > 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dice_graph = nx.from_pandas_edgelist(post_dice_coefs[['source', 'target', 'post_weight']], edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at one\n",
    "print(nx.to_dict_of_dicts(post_dice_graph).get('michie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights, we'll use them for plotting\n",
    "post_dice_weights = list(nx.get_edge_attributes(post_dice_graph,'post_weight').values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over values of resolution parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_resolution_iter = {}\n",
    "for r in [x / 10.0 for x in range(1, 50, 1)]:\n",
    "    comp = community_louvain.best_partition(post_dice_graph, weight='post_weight',  resolution=r)\n",
    "    Q = community_louvain.modularity(comp, post_dice_graph, weight='post_weight')\n",
    "    post_resolution_iter[r] = Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_resolution_iter   #1.0: 0.44476083753777085,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during\n",
    "post_partition = community_louvain.best_partition(post_dice_graph, weight='post_weight',  resolution=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Modularity (post network): {round(community_louvain.modularity(post_partition, post_dice_graph, weight='post_weight'), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nodes' betweeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nodes' degree to use as node size\n",
    "post_dice_graph_degrees = dict(nx.degree(post_dice_graph, weight='post_weight'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nodes' betweeness to use as node size\n",
    "post_betweenness_dict = nx.betweenness_centrality(\n",
    "    post_dice_graph, \n",
    "    weight='lock_weight'\n",
    "    ) \n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(post_dice_graph, post_betweenness_dict, 'betweenness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let color of edges start from a darker tone of gray\n",
    "#https://stackoverflow.com/questions/26102515/select-starting-color-in-matplotlib-colormap\n",
    "\n",
    "lvTmp_post = np.linspace(0.1,1.0,len(lock_dice_weights)-1)\n",
    "lvTmp_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmTmp_post = plt.cm.Greys(lvTmp_post)\n",
    "newCmap_post = mcol.ListedColormap(cmTmp_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (80,80))\n",
    "\n",
    "nx.draw_spring(post_dice_graph, \n",
    "               k=10,\n",
    "               with_labels=True, \n",
    "               edge_color=post_dice_weights,\n",
    "               width=[v*30 for v in post_dice_weights],\n",
    "               #node_color='lightgray',\n",
    "               node_color=list(post_partition.values()),\n",
    "               cmap=cmap,\n",
    "               linewidths=2,\n",
    "               #node_size=[v * 10000 for v in before_dice_graph_degrees.values()],\n",
    "               node_size=[(v * 100000)+2000 for v in post_betweenness_dict.values()],\n",
    "               font_size=100,\n",
    "               font_type=\"Helvetica\",\n",
    "               font_color=\"black\",\n",
    "               font_weight=3,\n",
    "               edge_cmap=newCmap,\n",
    "               edge_vmin=0,\n",
    "               seed=20\n",
    "              )\n",
    "\n",
    "plt.title(\"Post strict lockdown\".upper(), fontsize=150, **csfont)\n",
    "plt.axis('off')\n",
    " \n",
    "plt.savefig('post_network.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of the three networks and nodes\n",
    "\n",
    "Main ref: https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes (keywords that co-occured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of keywords co-occurring before-lockdown:\", len(before_dice_graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of keywords co-occurring during-lockdown:\", len(lock_dice_graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of keywords co-occurring post-lockdown:\", len(post_dice_graph.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network density\n",
    "= ratio between actual number of connections between nodes and maximum possible number of connections.\n",
    "\n",
    "Give a sense of how closely knit the network is, a higher value (within [0,1]) indicates a more cohesive network, so a set of keywords that do tend to co-occur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_density = nx.density(before_dice_graph)\n",
    "print(\"Network density (before lockdown):\", before_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_density = nx.density(lock_dice_graph)\n",
    "print(\"Network density (during lockdown):\", lock_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_density = nx.density(post_dice_graph)\n",
    "print(\"Network density (post lockdown):\", post_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network density has decreased during lockdown compared to pre-lockdown. \n",
    "    Interpretation: a decrease in the general tendency of keywords to co-occur together in the same documents. \n",
    "    \n",
    "- Network density decreased post lockdown compared to lockdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Clustering Coefficient\n",
    "\n",
    "= n^ of connections between the neighbour nodes of a node / maximum possible number of connections between its neighbour nodes\n",
    "\n",
    "(neighbour nodes are the nodes directly connected to a node).\n",
    "\n",
    "A measure of the degree to which nodes in a graph tend to cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_clustcoef = nx.average_clustering(before_dice_graph, weight='bef_weight')\n",
    "print(\"Network clustering coefficient (before lockdown):\", before_clustcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_clustcoef = nx.average_clustering(lock_dice_graph, weight='lock_weight')\n",
    "print(\"Network clustering coefficient (during lockdown):\", lock_clustcoef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_clustcoef = nx.average_clustering(post_dice_graph, weight='post_weight')\n",
    "print(\"Network clustering coefficient (post lockdown):\", post_clustcoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreases during lockdown compared to pre-lockdown also decreased in the post-lockdown compared to lockdown period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality measures\n",
    "\n",
    "Identify nodes (keywords) that are more important in the networks and compare the ranking them over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Degree\n",
    "\n",
    "The number of connection a node has. For a weighted network, this is the sum of the edge weights adjacent to the node. \n",
    "\n",
    "Here is with how many different keywords does each keyword co-occur?\n",
    "Note that this is likely to be proportional to the keyword's frequency. Something we can also report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_degree(graph, weight=None):\n",
    "    node_degree_dict = {}\n",
    "    for node in graph.nodes:\n",
    "        node_degree_dict[node] = nx.degree(graph, node, weight)\n",
    "    return node_degree_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_node_degrees = pd.Series(get_node_degree(before_dice_graph, weight='bef_weight')).sort_values(ascending=False)\n",
    "print(before_node_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_node_degrees = pd.Series(get_node_degree(lock_dice_graph, weight='lock_weight')).sort_values(ascending=False)\n",
    "print(lock_node_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_node_degrees = pd.Series(get_node_degree(post_dice_graph, weight='post_weight')).sort_values(ascending=False)\n",
    "print(post_node_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_degree_dict = dict(before_dice_graph.degree(before_dice_graph.nodes(), weight='bef_weight'))\n",
    "nx.set_node_attributes(before_dice_graph, before_degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_degree_dict = dict(lock_dice_graph.degree(lock_dice_graph.nodes(), weight='lock_weight'))\n",
    "nx.set_node_attributes(lock_dice_graph, lock_degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_degree_dict = dict(post_dice_graph.degree(post_dice_graph.nodes(), weight='post_weight'))\n",
    "nx.set_node_attributes(post_dice_graph, post_degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Betweeness Centrality\n",
    "\n",
    "Betweenness centrality doesn’t care about the number of edges any one node or set of nodes has. Betweenness centrality looks at all the shortest paths that pass through a particular node.\n",
    "\n",
    "So a keyword with a high betweeness centrality is a keyword that works as a bridge by connecting several different other keywords - i.e., it is discussed in articles with a wider variety of other keywords.\n",
    "\n",
    "https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html\n",
    "for weighted networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_betweenness_dict = nx.betweenness_centrality(\n",
    "    before_dice_graph, \n",
    "    weight='bef_weight'\n",
    "    ) \n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(before_dice_graph, before_betweenness_dict, 'betweenness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(before_betweenness_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare degree and between centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then find and print their degree\n",
    "for tb in sorted(before_betweenness_dict.items(), key=itemgetter(1), reverse=True): \n",
    "    degree = before_degree_dict[tb[0]] # Use degree_dict to access a node's degree\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### During lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_betweenness_dict = nx.betweenness_centrality(\n",
    "    lock_dice_graph,\n",
    "    weight='lock_weight') \n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(lock_dice_graph, lock_betweenness_dict, 'betweenness')\n",
    "\n",
    "\n",
    "sorted(lock_betweenness_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare degree and between centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tb in sorted(lock_betweenness_dict.items(), key=itemgetter(1), reverse=True): \n",
    "    degree = lock_degree_dict[tb[0]] # Use degree_dict to access a node's degree\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_betweenness_dict = nx.betweenness_centrality(\n",
    "    post_dice_graph,\n",
    "    weight='post_weight'\n",
    ") \n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(post_dice_graph, post_betweenness_dict, 'betweenness')\n",
    "\n",
    "\n",
    "sorted(post_betweenness_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tb in sorted(post_betweenness_dict.items(), key=itemgetter(1), reverse=True): \n",
    "    degree = post_degree_dict[tb[0]] # Use degree_dict to access a node's degree\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect community and re-draw networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Louivan algorithm for weighted undirected graphs\n",
    "\n",
    "Ref: https://python-louvain.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before\n",
    "\n",
    "bef_partition = community_louvain.best_partition(before_dice_graph, weight='bef_weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_partition.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(bef_partition.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap(cm.colors.ListedColormap([\"lightblue\", \"crimson\",  \"lightgreen\", \"pink\", \"orange\"]), \n",
    "                   max(bef_partition.values()) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color for the edges start from a darker tone of gray\n",
    "#https://stackoverflow.com/questions/26102515/select-starting-color-in-matplotlib-colormap\n",
    "\n",
    "lvTmp = np.linspace(0.1,1.0,len(before_dice_graph_weights)-1)\n",
    "lvTmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmTmp = plt.cm.Greys(lvTmp)\n",
    "newCmap = mcol.ListedColormap(cmTmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (80,80))\n",
    "\n",
    "nx.draw_spring(before_dice_graph, \n",
    "               k=10,\n",
    "               with_labels=True, \n",
    "               edge_color=before_dice_graph_weights,\n",
    "               width=[v*30 for v in before_dice_graph_weights],\n",
    "               #node_color='lightgray',\n",
    "               node_color=list(bef_partition.values()),\n",
    "               cmap=cmap,\n",
    "               linewidths=2,\n",
    "               #node_size=[v * 10000 for v in before_dice_graph_degrees.values()],\n",
    "               node_size=[(v * 100000)+2000 for v in before_betweenness_dict.values()],\n",
    "               font_size=100,\n",
    "               font_type=\"Helvetica\",\n",
    "               font_color=\"black\",\n",
    "               font_weight=3,\n",
    "               edge_cmap=newCmap,\n",
    "               edge_vmin=0,\n",
    "               seed=9\n",
    "              )\n",
    "\n",
    "plt.title(\"Before strict lockdown\".upper(), fontsize=150, **csfont)\n",
    "plt.axis('off')\n",
    " \n",
    "plt.savefig('output.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### During"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_partition = community_louvain.best_partition(lock_dice_graph, weight='lock_weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_partition = community_louvain.best_partition(post_dice_graph, weight='post_weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alterative: Using Girvan-Newman algorithm with edge-betweeness-centrality\n",
    "\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.centrality.girvan_newman.html#networkx.algorithms.community.centrality.girvan_newman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.pnas.org/content/101/9/2658\n",
    "Ref: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import edge_betweenness_centrality as betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import girvan_newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _most_central_edge(G, weights):\n",
    "    centrality = betweenness(G, weight=weights)\n",
    "    return max(centrality, key=centrality.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bef_most_central_edge(G):\n",
    "    return _most_central_edge(G, weights='bef_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_most_central_edge(before_dice_graph, weights='bef_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_gn = girvan_newman(before_dice_graph, \n",
    "                   most_valuable_edge=bef_most_central_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_comp = tuple(sorted(c) for c in next(bef_gn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### During"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_most_central_edge(G):\n",
    "    return _most_central_edge(G, weights='lock_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock_gn = girvan_newman(lock_dice_graph, \n",
    "                   most_valuable_edge=lock_most_central_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(sorted(c) for c in next(lock_gn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_most_central_edge(G):\n",
    "    return _most_central_edge(G, weights='post_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_gn = girvan_newman(post_dice_graph, \n",
    "                   most_valuable_edge=post_most_central_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(sorted(c) for c in next(post_gn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/ForceAtlas2/\n",
    "https://noduslabs.com/wp-content/uploads/2019/06/InfraNodus-Paranyushkin-WWW19-Conference.pdf\n",
    "\n",
    "We then apply community detection algorithm [37], [38] based\n",
    "on modularity. This is an iterative algorithm that detects the\n",
    "groups of nodes that are more densely connected together than\n",
    "with the rest of the network. As a result, we obtain the groups of\n",
    "nodes (words) which tend to appear together in the text: topical\n",
    "clusters. We then apply Force-Atlas algorithm [39], which aligns\n",
    "densely connected clusters together while pushing the most connected nodes apart, so that the network structure is more\n",
    "visible on the graph. We then get a visual network\n",
    "representation of the text with a clearly defined community\n",
    "structure (using both color and network topology) and the\n",
    "specific topical clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv': venv)",
   "language": "python",
   "name": "python38164bitvenvvenvdde169c8e66848db85d258e4e769d548"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
