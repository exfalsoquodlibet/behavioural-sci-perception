{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon-based sentiment analysis of tweets: using VADER and TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the sentiment score of the tweets using lexicon-based models VADER and TextBlob. Both are constructed from a generalizable, valence-based, human-curated gold standard sentiment lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "from src.preproc_text import *\n",
    "from src.utils import chain_functions\n",
    "from src.analyse_text import get_sentiment_score_VDR, get_sentiment_score_TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_seq_items = 10000\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment variables and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get(\"DIR_DATA_INTERIM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"tweets_relevant_keywords\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define domain-specific stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment requires context. \n",
    "\n",
    "When implementing an easy approach to sentiment analysis, you just have to kind of hope that you can ignore context and the sentiments will average out to the right trend.\n",
    "\n",
    "However, we can take context into account by excluding those terms that are sentiment-loaded but that in the covid-19 context are so common to be \"neutral\" (e.g., crisis, virus, pandemic). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRA_STOPWORDS = [\"coronavirus\", \"covid\", \"covid19\", \"covid-\", \"covid-19\" \"’s\", \"link\", \"dominic\", \"cummings\", \"boris\", \"johnson\", \n",
    "                   \"dr\", \"david\", \"halpern\", \"susan\", \"michie\", \"richard\", \"amlot\", \"thaler\", \"cass\", \"sunstein\", \n",
    "                   \"daniel\", \"kahneman\", \n",
    "                   \"d-\", \"th\", \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \n",
    "                   \"october\", \"november\", \"december\", \"corona\", \"virus\", \"wd\", \"&amp;\", \"article\", \"here\", \"%\", \"'s\",\n",
    "                  \"'ve'\", '&', 'amp', \"'re\", \"via\", \"hoe\", \"'ve\",\n",
    "                  \"crisis\", \"pandemic\", \"epidemic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(os.path.join(DATA_DIR, FILENAME + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there still duplicates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are still duplicates in the dataset that we need to get rid of. Consider re-teweet counts when doing do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a duplicate texts\n",
    "duplicate_tweets = tweets_df[tweets_df.duplicated(['text'])]\n",
    "print(duplicate_tweets[['favorite_count', 'retweet_count', 'text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently they are all duplicates of one single tweets.\n",
    "\n",
    "We will keep the one with the largest count of \"favourites\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_tweets[duplicate_tweets.favorite_count == max(duplicate_tweets.favorite_count)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index\n",
    "duplicate_tweets_index = duplicate_tweets[duplicate_tweets.favorite_count != \n",
    "                                          max(duplicate_tweets.favorite_count)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_tweets_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.drop(duplicate_tweets_index, axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick and dirty sentiment analysis without preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['quick_VDR_sentiment'] = [get_sentiment_score_VDR(tweet) for tweet in tweets_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['quick_TB_sentiment'] = [round(get_sentiment_score_TB(tweet),3) for tweet in tweets_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['text', 'quick_VDR_sentiment', 'quick_TB_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many negative tweets have not been captured as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 100 cases from tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets = tweets_df.sample(n=120, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets[['created_at', 'id', 'user_location', \n",
    "               'relevant_subkwords', 'text']].to_csv(os.path.join(DATA_DIR, \"sample1_tweets.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First part\n",
    "\n",
    "1. Replace emojis and emoticons with corresponding text description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second part \n",
    "\n",
    "2. Replace URLs with word \"url\" or remove them\n",
    "3. Remove first name of users metioned or replace them with \"user_mentioned\"\n",
    "4. Replace all the hashtags with the words with the hash symbol (e.g., \"#hello\" -> \"hello\")\n",
    "\n",
    "Given that 1. and 2. are not part of the lexicons, so do not contribute to the sentiment score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third part\n",
    "\n",
    "5. Split compounded-by-upper-case strings\n",
    "6. Split compounded-by-underscore \"_\" strings (this is two get the words that make up the emojis descriptions)\n",
    "7. Remove digits\n",
    "8. Remove single-character words\n",
    "9. Split domain-specific compunded all-lower-case strings (e.g., behaviouraleconomics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth part\n",
    "\n",
    "10. Remove stop words\n",
    "11. Remove punctuation (but keep !?...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a check: a sample of tweets that contain emojis\n",
    "idx_sample_tweets_emojs = [37, 57, 135, 136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [convert_emojis(t) for t in tweets_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [convert_emoticons(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "[tweets[i] for i in idx_sample_tweets_emojs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [clean_tweet_quibbles(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tweets[i] for i in idx_sample_tweets_emojs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe1 = chain_functions(split_lowercase_compounds,\n",
    "                                split_string_at_uppercase,\n",
    "                                break_compound_words,\n",
    "                                remove_digits, \n",
    "                                remove_single_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [preproc_pipe1(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tweets[i] for i in idx_sample_tweets_emojs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower text\n",
    "tweets = [tweet.lower() for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise_pipe = chain_functions(tokenise_sent, tokenise_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tok = [tokenise_pipe(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_tok = [remove_stopwords(tweet, extra_stopwords= EXTRA_STOPWORDS) for tweet in tweets_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "[tweets_tok[idx] for idx in idx_sample_tweets_emojs][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do not remove punctuation form the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2string_pipe = chain_functions(flatten_irregular_listoflists, list, detokenise_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cleaned = [tokens2string_pipe(tweet) for tweet in tweets_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra white space before punctuation\n",
    "tweets_cleaned = [re.sub(r'\\s([?.!,;:\"](?:\\s|$))', r'\\1', tweet) for tweet in tweets_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look\n",
    "[tweets_cleaned[i] for i in idx_sample_tweets_emojs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge to original dataset of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet_cleaned'] = tweets_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['text', 'tweet_cleaned']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER sentiment analysis on cleaned-text tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER stands for Valence Aware Dictionary for Sentiment Reasoning and is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. \n",
    "\n",
    "Intro to VADER: https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['VDR_sentiment'] = [get_sentiment_score_VDR(tweet) for tweet in tweets_df.tweet_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['text', 'tweet_cleaned','VDR_sentiment']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob sentiment analysis on cleaned-text tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to TextBlob: https://planspace.org/20150607-textblob_sentiment/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['TB_sentiment'] = [round(get_sentiment_score_TB(tweet),3) for tweet in tweets_df.tweet_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['text', 'tweet_cleaned','VDR_sentiment', 'TB_sentiment']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these scores to the ones obtained for the non-preprocessed tweet texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER with individual score for pos/neu/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['VDR_detailed_sentiment'] = [get_sentiment_score_VDR(tweet, score_type='all') for tweet in tweets_df.tweet_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['text', 'tweet_cleaned','VDR_sentiment', 'VDR_detailed_sentiment', 'TB_sentiment']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's clean the text less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try not to clean the text as much as Vader should be sensitive to emoticons, capital letter that emphasise, etc... See: https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n",
    "\n",
    "So, we will:\n",
    "\n",
    "- keep emojis and emoticons in as they are\n",
    "- not lemmatise\n",
    "- not lower-case\n",
    "- not remove stop-words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New cleaning steps\n",
    "\n",
    "1. Replace URLs with word \"url\" or remove them\n",
    "2. Remove first name of users metioned or replace them with \"user_mentioned\"\n",
    "3. Replace all the hashtags with the words with the hash symbol (e.g., \"#hello\" -> \"hello\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split compounded-by-upper-case strings\n",
    "5. Split compounded-by-underscore \"_\" strings (this is two get the words that make up the emojis descriptions)\n",
    "6. Remove digits\n",
    "7. Remove single-character words\n",
    "8. Split domain-specific compunded all-lower-case strings (e.g., behaviouraleconomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2 = [clean_tweet_quibbles(tweet) for tweet in tweets_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2 = [preproc_pipe1(tweet) for tweet in tweets_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet_cleaned_less'] = tweets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['VDR_sentiment_2'] = [get_sentiment_score_VDR(tweet) for tweet in tweets_df.tweet_cleaned_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['VDR_detailed_sentiment_2'] = [get_sentiment_score_VDR(tweet, score_type='all') for tweet in tweets_df.tweet_cleaned_less]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[['text', 'tweet_cleaned_less','VDR_sentiment', 'VDR_sentiment_2', 'VDR_detailed_sentiment', 'VDR_detailed_sentiment_2']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much difference, really."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset with sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_to_save = tweets_df[['id', 'created_at', 'favorite_count', 'retweet_count', \n",
    "           'text', 'tweet_cleaned', 'tweet_cleaned_less', 'VDR_sentiment', 'VDR_sentiment_2', 'TB_sentiment', 'VDR_detailed_sentiment', 'VDR_detailed_sentiment_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_to_save.to_csv(os.path.join(DATA_DIR, \"tweets_en_lexicon_sentiments.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_to_save.to_pickle(os.path.join(DATA_DIR, \"tweets_en_lexicon_sentiments.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
